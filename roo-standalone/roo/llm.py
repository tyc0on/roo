"""
Model-Agnostic LLM Client

Supports multiple LLM providers with a unified async interface.
"""
from abc import ABC, abstractmethod
from dataclasses import dataclass
from typing import Optional, List, Dict, Any
from enum import Enum

from .config import get_settings


class LLMProvider(str, Enum):
    """Supported LLM providers."""
    GEMINI = "gemini"
    OPENAI = "openai"
    ANTHROPIC = "anthropic"


@dataclass
class LLMResponse:
    """Standardized response from LLM."""
    content: str
    model: str
    usage: Optional[Dict[str, int]] = None


class BaseLLMClient(ABC):
    """Abstract base class for LLM clients."""
    
    @abstractmethod
    async def chat(self, messages: List[Dict[str, str]], **kwargs) -> LLMResponse:
        """Send a chat completion request."""
        pass
    
    @abstractmethod
    async def embed(self, text: str) -> List[float]:
        """Generate embeddings for text."""
        pass


class OpenAIClient(BaseLLMClient):
    """Client for OpenAI API (also used for Gemini via compatibility layer)."""
    
    def __init__(self, api_key: str, model: str, base_url: Optional[str] = None):
        from openai import AsyncOpenAI
        
        self.model = model
        self.client = AsyncOpenAI(api_key=api_key, base_url=base_url)
    
    async def chat(self, messages: List[Dict[str, str]], **kwargs) -> LLMResponse:
        """Send chat completion request."""
        response = await self.client.chat.completions.create(
            model=self.model,
            messages=messages,
            temperature=kwargs.get("temperature", 0.7),
            max_tokens=kwargs.get("max_tokens", 2048)
        )
        
        return LLMResponse(
            content=response.choices[0].message.content or "",
            model=response.model,
            usage={
                "prompt_tokens": response.usage.prompt_tokens if response.usage else 0,
                "completion_tokens": response.usage.completion_tokens if response.usage else 0
            }
        )
    
    async def embed(self, text: str) -> List[float]:
        """Generate embeddings using OpenAI."""
        response = await self.client.embeddings.create(
            model="text-embedding-ada-002",
            input=text
        )
        return response.data[0].embedding


class AnthropicClient(BaseLLMClient):
    """Client for Anthropic Claude API."""
    
    def __init__(self, api_key: str, model: str = "claude-3-5-sonnet-20241022"):
        from anthropic import AsyncAnthropic
        
        self.model = model
        self.client = AsyncAnthropic(api_key=api_key)
    
    async def chat(self, messages: List[Dict[str, str]], **kwargs) -> LLMResponse:
        """Send chat completion request to Claude."""
        # Extract system message if present
        system = None
        chat_messages = []
        
        for msg in messages:
            if msg["role"] == "system":
                system = msg["content"]
            else:
                chat_messages.append(msg)
        
        response = await self.client.messages.create(
            model=self.model,
            max_tokens=kwargs.get("max_tokens", 2048),
            system=system or "You are a helpful assistant.",
            messages=chat_messages
        )
        
        return LLMResponse(
            content=response.content[0].text if response.content else "",
            model=response.model,
            usage={
                "input_tokens": response.usage.input_tokens,
                "output_tokens": response.usage.output_tokens
            }
        )
    
    async def embed(self, text: str) -> List[float]:
        """Claude doesn't support embeddings, fall back to OpenAI."""
        settings = get_settings()
        if settings.OPENAI_API_KEY:
            client = OpenAIClient(settings.OPENAI_API_KEY, "text-embedding-ada-002")
            return await client.embed(text)
        raise ValueError("OpenAI API key required for embeddings with Anthropic")


# Default configurations
DEFAULT_CONFIGS = {
    LLMProvider.GEMINI: {
        "model": "gemini-2.5-flash",
        "base_url": "https://generativelanguage.googleapis.com/v1beta/openai/",
    },
    LLMProvider.OPENAI: {
        "model": "gpt-4o-mini",
        "base_url": None,
    },
    LLMProvider.ANTHROPIC: {
        "model": "claude-3-5-sonnet-20241022",
        "base_url": None,
    },
}


def get_llm_client(provider: Optional[str] = None) -> BaseLLMClient:
    """
    Factory function to get an LLM client.
    
    Args:
        provider: Provider name ("gemini", "openai", "anthropic")
                  Auto-detects based on available API keys if not specified
    
    Returns:
        LLM client instance
    """
    settings = get_settings()
    
    # Auto-detect provider if not specified
    if provider is None:
        provider = settings.default_llm_provider
    
    provider_enum = LLMProvider(provider.lower())
    config = DEFAULT_CONFIGS[provider_enum]
    
    if provider_enum == LLMProvider.GEMINI:
        if not settings.GOOGLE_API_KEY:
            raise ValueError("GOOGLE_API_KEY not configured")
        return OpenAIClient(
            api_key=settings.GOOGLE_API_KEY,
            model=config["model"],
            base_url=config["base_url"]
        )
    
    if provider_enum == LLMProvider.OPENAI:
        if not settings.OPENAI_API_KEY:
            raise ValueError("OPENAI_API_KEY not configured")
        return OpenAIClient(
            api_key=settings.OPENAI_API_KEY,
            model=config["model"]
        )
    
    if provider_enum == LLMProvider.ANTHROPIC:
        if not settings.ANTHROPIC_API_KEY:
            raise ValueError("ANTHROPIC_API_KEY not configured")
        return AnthropicClient(
            api_key=settings.ANTHROPIC_API_KEY,
            model=config["model"]
        )
    
    raise ValueError(f"Unknown provider: {provider}")


# Singleton client
_default_client: Optional[BaseLLMClient] = None


def get_default_client() -> BaseLLMClient:
    """Get or create the default LLM client."""
    global _default_client
    if _default_client is None:
        _default_client = get_llm_client()
    return _default_client


async def chat(messages: List[Dict[str, str]], **kwargs) -> LLMResponse:
    """Convenience function for quick chat completions."""
    client = get_default_client()
    return await client.chat(messages, **kwargs)


async def embed(text: str) -> List[float]:
    """Convenience function for generating embeddings."""
    client = get_default_client()
    return await client.embed(text)
